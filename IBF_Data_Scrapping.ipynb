{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eddb5d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL https://www.amazon.ca/s?k=mens white shirt&sprefix=fash%2Caps%2C83&ref=nb_sb_ss_ts-doa-p_2_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bt/pxjs738143scynr3mqx9zzww0000gn/T/ipykernel_16213/640593095.py:34: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"C:\\\\Users\\\\HP1\\\\Downloads\\\\driver\\\\chromedriver.exe\", options=options)  # path=r'to/chromedriver.exe'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN NoSuchElementException\n",
      "IN FINALLY\n",
      "IN FINALLY\n",
      "IN FINALLY\n",
      "IN FINALLY\n",
      "IN FINALLY\n",
      "IN FINALLY\n",
      "IN FINALLY\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import csv\n",
    "\n",
    "#  Any infinity scroll URL\n",
    "mainURL = \"https://www.amazon.ca/\"\n",
    "var = \"mens white shirt\"\n",
    "\n",
    "# url = \"https://in.pinterest.com/search/pins/?q=\" + var \n",
    "\n",
    "url = \"https://www.amazon.ca/s?k=\"+ var + \"&sprefix=fash%2Caps%2C83&ref=nb_sb_ss_ts-doa-p_2_4\" \n",
    "print(\"URL\",url);\n",
    "\n",
    "sleepTimer = 3    # Waiting 1 second for page to load\n",
    "image_limits = 500\n",
    "images_per_page = []\n",
    "no_of_images = []\n",
    "\n",
    "images = []\n",
    "title = []\n",
    "price = []\n",
    "\n",
    "#  Bluetooth bug circumnavigate\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "\n",
    "driver = webdriver.Chrome(\"C:\\\\Users\\\\HP1\\\\Downloads\\\\driver\\\\chromedriver.exe\", options=options)  # path=r'to/chromedriver.exe'\n",
    "driver.get(url)\n",
    "\n",
    "csv_data = []\n",
    "\n",
    "def scroolWindow(): \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\");\n",
    "    time.sleep(sleepTimer)\n",
    "    scrapeImages()\n",
    "    \n",
    "def scrapeImages():\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    for (i, divDom) in enumerate(soup.find_all('div', {'class': 'a-spacing-base'})):\n",
    "\n",
    "        if divDom.find('h2') and divDom.find('span', {'class': 'a-offscreen'}) and divDom.find_all('img') and not divDom.find('div', {'class': 'sbv-product-container'}):\n",
    "            if divDom.find('h2').text not in title:\n",
    "                title.append(divDom.find('h2').text);\n",
    "                price.append(divDom.find('span', {'class': 'a-offscreen'}).text);\n",
    "                for (i, link) in enumerate(divDom.find_all('img')):\n",
    "                    no_of_images.append(link.get('src'))\n",
    "#                     images_per_page.append(link.get('src'))\n",
    "                    img_data = requests.get(link.get('src')).content\n",
    "                    with open(f'{len(no_of_images)} image.jpg', 'wb') as handler:\n",
    "                        csv_data.append([(len(csv_data) + 1), f'{len(no_of_images)} image.jpg', divDom.find('h2').text, divDom.find('span', {'class': 'a-offscreen'}).text])\n",
    "                        handler.write(img_data)\n",
    "     \n",
    "    if len(no_of_images) < image_limits:\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\")\n",
    "            next_button.click()\n",
    "            scroolWindow()\n",
    "        except NoSuchElementException:\n",
    "            # Handle the exception\n",
    "            # Save the scraped data to a CSV file\n",
    "            print(\"IN NoSuchElementException\")\n",
    "            with open('amazon scraped '+var+' data.csv', 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['ID', 'Image', 'Title', 'Price'])\n",
    "                for row in csv_data:\n",
    "                    writer.writerow(row)\n",
    "        finally:\n",
    "            # Quit the webdriver instance\n",
    "            print(\"IN FINALLY\")\n",
    "            with open('amazon scraped '+var+' data.csv', 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['ID', 'Image', 'Title', 'Price'])\n",
    "                for row in csv_data:\n",
    "                    writer.writerow(row)\n",
    "            driver.quit()\n",
    "\n",
    "scrapeImages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8d590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c5e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
